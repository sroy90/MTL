{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61abea1b",
   "metadata": {},
   "source": [
    "# This script processes a citation network text file \n",
    "And builds a dictionary mapping each paper's unique `#index` to a list of paper indices it references (i.e., citations). It includes logic to ensure that only papers with authors listed are considered, and references (`#%`) are only counted for such papers. The final mapping is saved in mapping_idx.pkl file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1756547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 entries\n",
      "Processed 200000 entries\n",
      "Processed 300000 entries\n",
      "Processed 400000 entries\n",
      "Processed 500000 entries\n",
      "Processed 600000 entries\n",
      "Processed 700000 entries\n",
      "Processed 800000 entries\n",
      "Processed 900000 entries\n",
      "Processed 1000000 entries\n",
      "Processed 1100000 entries\n",
      "Processed 1200000 entries\n",
      "Processed 1300000 entries\n",
      "Saved 1329989 entries to mapping_idx.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def parse_txt_to_dict_streaming(input_file, output_file):\n",
    "    data_dict = {}\n",
    "    current_refs = []\n",
    "    has_authors = False\n",
    "    current_index = None\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # skip first line\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#*'):\n",
    "                current_refs = []\n",
    "                has_authors = False\n",
    "                current_index = None\n",
    "            elif line.startswith('#@'):\n",
    "                if line != '#@':\n",
    "                    has_authors = True\n",
    "            elif line.startswith('#%'):\n",
    "                if has_authors:\n",
    "                    ref = line[2:].strip()\n",
    "                    if ref.isdigit():\n",
    "                        current_refs.append(int(ref))\n",
    "            elif line.startswith('#index'):\n",
    "                if has_authors:\n",
    "                    current_index = int(line[6:].strip())\n",
    "                    data_dict[current_index] = current_refs\n",
    "\n",
    "                    # Optional: periodically dump to disk to free memory\n",
    "                    if len(data_dict) % 100000 == 0:\n",
    "                        print(f\"Processed {len(data_dict)} entries\")\n",
    "\n",
    "    # Save the final dictionary\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(data_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"Saved {len(data_dict)} entries to {output_file}\")\n",
    "\n",
    "parse_txt_to_dict_streaming('citation-network2.txt', 'mapping_idx.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d7317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: []\n",
      "1: [774794, 95940]\n",
      "3: [858446, 435642, 1293715, 412124, 414766, 1301929, 1102537, 102223]\n",
      "4: []\n",
      "5: []\n",
      "6: []\n",
      "7: []\n",
      "8: [378882]\n",
      "9: [684494, 439170, 495494, 794270, 911369, 800580, 376413, 102495, 511257, 423180, 402729]\n",
      "10: []\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open('mapping_idx.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "# Print top 10 items (key-value pairs)\n",
    "for i, (key, value) in enumerate(data_dict.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f4a1b",
   "metadata": {},
   "source": [
    "# Remove entries with empty value lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1271b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dictionary saved with 386497 entries.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dictionary\n",
    "with open(\"mapping_idx.pkl\", \"rb\") as f:\n",
    "    mapping_idx = pickle.load(f)\n",
    "\n",
    "# Remove entries with empty value lists\n",
    "filtered_mapping = {k: v for k, v in mapping_idx.items() if v}\n",
    "\n",
    "# Save the filtered dictionary\n",
    "with open(\"mapping_idx_filtered.pkl\", \"wb\") as f:\n",
    "    pickle.dump(filtered_mapping, f)\n",
    "\n",
    "print(f\"Filtered dictionary saved with {len(filtered_mapping)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db2377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [774794, 95940]\n",
      "3: [858446, 435642, 1293715, 412124, 414766, 1301929, 1102537, 102223]\n",
      "8: [378882]\n",
      "9: [684494, 439170, 495494, 794270, 911369, 800580, 376413, 102495, 511257, 423180, 402729]\n",
      "13: [1284256, 407367, 1294040, 429402, 354224, 124237, 290792]\n",
      "24: [1288569, 806859, 628820, 636684, 1300010, 301407]\n",
      "25: [629319, 123113, 775152, 647357, 622012, 894102, 775711, 645080]\n",
      "27: [5032]\n",
      "31: [166201, 988854, 109276, 783542, 124393, 292880, 152144, 392657, 434387, 311159, 106860]\n",
      "34: [406408, 301680, 950201]\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 entries for verification\n",
    "for i, (k, v) in enumerate(filtered_mapping.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbad0f7",
   "metadata": {},
   "source": [
    "**processes citation network text file and creates a dictionary mapping each paper's unique `#index` to its list of authors from the `#@` field. Only entries with non-empty author fields are considered. The resulting dictionary is saved to a `author_mapping.pkl` file for later use.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b26a5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1329989 records to 'author_mapping.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def parse_authors_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Ignore the first line\n",
    "    lines = lines[1:]\n",
    "\n",
    "    current_authors = None\n",
    "    current_index = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"#@\"):\n",
    "            current_authors = line[2:].strip()\n",
    "        elif line.startswith(\"#index\"):\n",
    "            current_index = int(line[6:].strip())\n",
    "            if current_authors:  # Only if authors exist\n",
    "                data_dict[current_index] = current_authors\n",
    "            # Reset for next record\n",
    "            current_authors = None\n",
    "            current_index = None\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# File path to your input text file\n",
    "input_file = \"citation-network2.txt\"  \n",
    "output_file = \"author_mapping.pkl\"\n",
    "\n",
    "# Create dictionary and save to .pkl\n",
    "author_dict = parse_authors_to_dict(input_file)\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(author_dict, f)\n",
    "\n",
    "print(f\"Saved {len(author_dict)} records to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0c16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: E. S. Cho,C. J. Kim,S. D. Kim,S. Y. Rhew\n",
      "1: Lori M. Weber,Alysha Loumakis,James Bergman\n",
      "3: Choong-Gyoo Lim\n",
      "4: Jose Maria Perez,Felix Garcia,Jesus Carretero,Alejandro Calderon,Luis Miguel Sanchez\n",
      "5: Jean Kumagai\n",
      "6: Marek Rusinkiewicz,Dimitrios Georgakopoulos\n",
      "7: Barton C. Massey,Evan Tick\n",
      "8: Jan Ramon\n",
      "9: Therapon Skotiniotis,Ji-en Morris Chang\n",
      "10: V. Martin,K. Schwan\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open('author_mapping.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "# Print top 10 items (key-value pairs)\n",
    "for i, (key, value) in enumerate(data_dict.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "    if i == 9:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00cb38",
   "metadata": {},
   "source": [
    "**Filters an existing author-to-paper mapping (`author_mapping.pkl`) using a secondary file (`AMiner-Author.txt`) that contains detailed author metadata including affiliation. Only authors who have non-empty affiliations (`#a`) are retained in the final dictionary. The result is saved to a new `remove_author_#a.pkl` file.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d982d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered author mapping saved with 1036217 entries.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load author mapping dictionary\n",
    "with open(\"author_mapping.pkl\", \"rb\") as f:\n",
    "    author_mapping = pickle.load(f)\n",
    "\n",
    "# Path to the large .txt file containing paper and author info\n",
    "txt_file_path = \"AMiner-Author.txt\"\n",
    "\n",
    "# Preprocessing:\n",
    "# We'll parse the txt file once and build a mapping: author_name -> affiliation (empty if none)\n",
    "author_affiliation = {}\n",
    "\n",
    "with open(txt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    current_author = None\n",
    "    current_affiliation = None\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"#index\"):\n",
    "            # reset for new paper\n",
    "            current_author = None\n",
    "            current_affiliation = None\n",
    "        elif line.startswith(\"#n \"):\n",
    "            current_author = line[3:].strip()\n",
    "        elif line.startswith(\"#a \"):\n",
    "            current_affiliation = line[3:].strip()\n",
    "            if current_author is not None:\n",
    "                author_affiliation[current_author] = current_affiliation\n",
    "\n",
    "# Now filter authors from author_mapping using author_affiliation\n",
    "filtered_author_mapping = {}\n",
    "\n",
    "for key, authors_str in author_mapping.items():\n",
    "    authors = [a.strip() for a in authors_str.split(\",\") if a.strip()]\n",
    "    filtered_authors = []\n",
    "    for author in authors:\n",
    "        aff = author_affiliation.get(author)\n",
    "        if aff and aff.strip():\n",
    "            filtered_authors.append(author)\n",
    "    if filtered_authors:\n",
    "        filtered_author_mapping[key] = \", \".join(filtered_authors)\n",
    "    # else key is skipped (all authors removed)\n",
    "\n",
    "# Save filtered author mapping\n",
    "with open(\"remove_author_#a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(filtered_author_mapping, f)\n",
    "\n",
    "print(f\"Filtered author mapping saved with {len(filtered_author_mapping)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60715ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: C. J. Kim, S. D. Kim\n",
      "1: Lori M. Weber, Alysha Loumakis, James Bergman\n",
      "3: Choong-Gyoo Lim\n",
      "4: Felix Garcia, Jesus Carretero, Luis Miguel Sanchez\n",
      "5: Jean Kumagai\n",
      "6: Marek Rusinkiewicz\n",
      "7: Evan Tick\n",
      "8: Jan Ramon\n",
      "9: Therapon Skotiniotis, Ji-en Morris Chang\n",
      "10: V. Martin, K. Schwan\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 entries for verification\n",
    "for i, (k, v) in enumerate(filtered_author_mapping.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b203d3",
   "metadata": {},
   "source": [
    "**performs a final filtering step by intersecting a paper-to-author index mapping (`mapping_idx_filtered.pkl`) with a filtered author list (`remove_author_#a.pkl`) that contains only authors with valid affiliations. The resulting dictionary keeps only those entries where both the paper and the authors are validated, and saves it as `final_mapping.pkl`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8b3eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mapping saved with 348882 keys.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dictionaries\n",
    "with open(\"mapping_idx_filtered.pkl\", \"rb\") as f:\n",
    "    mapping_idx_filtered = pickle.load(f)\n",
    "\n",
    "with open(\"remove_author_#a.pkl\", \"rb\") as f:\n",
    "    remove_author_a = pickle.load(f)\n",
    "\n",
    "# Filter the dictionary\n",
    "final_mapping = {}\n",
    "\n",
    "for key, authors in mapping_idx_filtered.items():\n",
    "    # Keep key only if it's present in remove_author_a (as a key)\n",
    "    if key not in remove_author_a:\n",
    "        continue\n",
    "\n",
    "    # Filter author list to only those present in remove_author_a\n",
    "    filtered_authors = [author for author in authors if author in remove_author_a]\n",
    "\n",
    "    # If filtered list is not empty, keep it\n",
    "    if filtered_authors:\n",
    "        final_mapping[key] = filtered_authors\n",
    "\n",
    "# Save the final filtered mapping\n",
    "with open(\"final_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_mapping, f)\n",
    "\n",
    "print(f\"Final mapping saved with {len(final_mapping)} keys.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2811444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [774794, 95940]\n",
      "3: [858446, 435642, 412124, 414766, 1301929, 1102537, 102223]\n",
      "8: [378882]\n",
      "9: [684494, 439170, 495494, 800580, 102495, 511257, 402729]\n",
      "13: [407367, 1294040, 429402, 354224, 124237, 290792]\n",
      "24: [1288569, 806859, 628820, 636684, 1300010, 301407]\n",
      "25: [629319, 123113, 775152, 647357, 622012, 775711, 645080]\n",
      "27: [5032]\n",
      "31: [109276, 783542, 124393, 292880, 392657, 434387, 106860]\n",
      "34: [406408, 301680, 950201]\n",
      "[102229, 512523, 401309, 158212, 118593, 498294, 311269, 685389]\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(final_mapping.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "# print(data_dict[774794])\n",
    "print(final_mapping[108893])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60935ff4",
   "metadata": {},
   "source": [
    "**performs a Depth-First Search (DFS) traversal on a citation graph stored in `final_mapping.pkl`, starting from each of the first `max_nodes` papers. It collects all reachable papers (multi-hop references) and stores the result in `multi_hop_first_11000.pkl`. This helps in understanding multi-hop citation chains across papers.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b85c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 keys...\n",
      "Processed 2000 keys...\n",
      "Processed 3000 keys...\n",
      "Processed 4000 keys...\n",
      "Processed 5000 keys...\n",
      "Processed 6000 keys...\n",
      "Processed 7000 keys...\n",
      "Processed 8000 keys...\n",
      "Processed 9000 keys...\n",
      "Processed 10000 keys...\n",
      "Processed 11000 keys...\n",
      "\n",
      "DFS completed for 11000 keys. Results saved to 'multi_hop_first_11000.pkl'\n",
      "Total papers processed: 11000\n",
      "Example result for first key: (1, [95940, 774794])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the input graph\n",
    "with open(\"final_mapping.pkl\", \"rb\") as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "# Parameters\n",
    "max_nodes = 11000  # Only process first max_nodes(<22000) nodes\n",
    "print_interval = 1000  # Print progress every 1000 nodes\n",
    "output_file = \"multi_hop_first_11000.pkl\"  # Single output file\n",
    "\n",
    "results = {}\n",
    "processed = 0\n",
    "\n",
    "for start_key in list(graph.keys())[:max_nodes]:  # Only take first max_nodes(<22000) keys\n",
    "    visited = set()\n",
    "    stack = [start_key]\n",
    "    result = set()\n",
    "\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        if node != start_key:\n",
    "            result.add(node)\n",
    "\n",
    "        for neighbor in graph.get(node, []):\n",
    "            if neighbor not in visited:\n",
    "                stack.append(neighbor)\n",
    "\n",
    "    results[start_key] = sorted(result)\n",
    "    processed += 1\n",
    "\n",
    "    # Print progress\n",
    "    if processed % print_interval == 0:\n",
    "        print(f\"Processed {processed} keys...\")\n",
    "    \n",
    "    # Early exit if we've reached our limit\n",
    "    if processed >= max_nodes:\n",
    "        break\n",
    "\n",
    "# Save all results to a single file\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"\\nDFS completed for {processed} keys. Results saved to '{output_file}'\")\n",
    "print(f\"Total papers processed: {len(results)}\")\n",
    "print(f\"Example result for first key: {list(results.items())[0] if results else 'No results'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c21290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"multi_hop_first_11000.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "# for i, (k, v) in enumerate(test.items()):\n",
    "#     print(f\"{k}: {v}\")\n",
    "#     if k == 9:\n",
    "#         break\n",
    "    \n",
    "# print(data_dict[774794])\n",
    "# print(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda4f9e",
   "metadata": {},
   "source": [
    "# Ensures a complete mapping of paper IDs to their first authors by:\n",
    "\n",
    "1. **Loading existing mappings** from `mapping_idx.pkl`, which maps paper IDs to author strings.\n",
    "2. **Extracting and storing the first author** from each author string.\n",
    "3. **Parsing the original `citation-network2.txt` file** to find additional paper IDs not in the initial mapping.\n",
    "4. **For each new paper**, it scans backwards from `#index` to find its `#@` author line and extracts the first author.\n",
    "5. **Updates the mapping** with these new entries, resulting in a more complete `id_author_mapping`.\n",
    "\n",
    "This is useful for ensuring every paper (with authors listed) has a known first author for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85309abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total IDs collected: 1329989\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Step 1: Load mapping_idx.pkl and extract first authors\n",
    "with open(\"mapping_idx.pkl\", \"rb\") as f:\n",
    "    mapping_data = pickle.load(f)\n",
    "\n",
    "id_author_mapping = {}\n",
    "\n",
    "# Extract first author from existing mapping\n",
    "for paper_id, authors in mapping_data.items():\n",
    "    if isinstance(authors, str) and authors.strip():\n",
    "        first_author = authors.split(\",\")[0].strip()\n",
    "        id_author_mapping[int(paper_id)] = first_author\n",
    "\n",
    "# Step 2: Open citation-network2.txt and add new IDs with first authors if not already present\n",
    "with open(\"citation-network2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    line = lines[i].strip()\n",
    "\n",
    "    if line.startswith(\"#index\"):\n",
    "        paper_id = int(line.replace(\"#index\", \"\").strip())\n",
    "\n",
    "        # Check if ID already exists\n",
    "        if paper_id not in id_author_mapping:\n",
    "            # Look for #@ line (authors)\n",
    "            j = i - 1\n",
    "            first_author = None\n",
    "\n",
    "            # Walk backwards to find #@ line for current paper\n",
    "            while j >= 0:\n",
    "                author_line = lines[j].strip()\n",
    "                if author_line.startswith(\"#@\"):\n",
    "                    author_data = author_line.replace(\"#@\", \"\").strip()\n",
    "                    if author_data:\n",
    "                        first_author = author_data.split(\",\")[0].strip()\n",
    "                    break\n",
    "                j -= 1\n",
    "\n",
    "            # Add if valid first author found\n",
    "            if first_author:\n",
    "                id_author_mapping[paper_id] = first_author\n",
    "\n",
    "    i += 1\n",
    "\n",
    "#  Final Output\n",
    "print(\" Total IDs collected:\", len(id_author_mapping))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7423a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: E. S. Cho\n",
      "1: Lori M. Weber\n",
      "3: Choong-Gyoo Lim\n",
      "4: Jose Maria Perez\n",
      "5: Jean Kumagai\n",
      "6: Marek Rusinkiewicz\n",
      "7: Barton C. Massey\n",
      "8: Jan Ramon\n",
      "9: Therapon Skotiniotis\n",
      "10: V. Martin\n",
      "id_author_mapping has been saved to id_author_mapping.pkl\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(id_author_mapping.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the dictionary to id_author_mapping.pkl\n",
    "with open(\"id_author_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_author_mapping, f)\n",
    "\n",
    "print(\"id_author_mapping has been saved to id_author_mapping.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d9d58",
   "metadata": {},
   "source": [
    "# Processes multi-hop citation data and converts paper IDs into author names.\n",
    "\n",
    "**Workflow:**\n",
    "1. Loads a paper ID â†’ author name mapping (`id_author_mapping.pkl`).\n",
    "2. Loads a dictionary (`multi_hop_first_11000.pkl`) mapping each paper ID to a list of reachable paper IDs (multi-hop citations).\n",
    "3. For each paper:\n",
    "   - Maps the source paper and reachable papers to author names.\n",
    "   - Excludes self-citations (same author).\n",
    "   - Gathers a list of *unique* cited authors.\n",
    "4. Saves the final mapping: `source_author â†’ list of unique reachable authors` into `multi_hop_author_name_11000.pkl`.\n",
    "\n",
    "**Output:** A cleaned author-level multi-hop influence map, filtering out missing or self-cited authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66956df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing multi_hop_first_11000.pkl...\n",
      "Processing complete. Results saved to multi_hop_author_name_11000.pkl\n",
      "Total authors processed: 9967\n",
      "Example mapping: ('Lori M. Weber', ['James C. Witte', 'John P. Robinson'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load id to author name mapping\n",
    "with open(\"id_author_mapping.pkl\", \"rb\") as f:\n",
    "    id_author_mapping = pickle.load(f)\n",
    "\n",
    "# Single file to process (change this to your target file)\n",
    "input_file = \"multi_hop_first_11000.pkl\"  \n",
    "output_file = \"multi_hop_author_name_11000.pkl\"\n",
    "\n",
    "print(f\"Processing {input_file}...\")\n",
    "\n",
    "# Load the multi-hop data\n",
    "with open(input_file, \"rb\") as f:\n",
    "    multi_hop_data = pickle.load(f)\n",
    "\n",
    "author_hop_data = {}\n",
    "\n",
    "for paper_id, reachable_papers in multi_hop_data.items():\n",
    "    # Skip if paper_id has no author mapping\n",
    "    if paper_id not in id_author_mapping:\n",
    "        continue\n",
    "    \n",
    "    source_author = id_author_mapping[paper_id]\n",
    "    unique_authors = set()\n",
    "    \n",
    "    for cited_paper in reachable_papers:\n",
    "        if cited_paper in id_author_mapping:\n",
    "            cited_author = id_author_mapping[cited_paper]\n",
    "            # Avoid self-citations\n",
    "            if cited_author != source_author:\n",
    "                unique_authors.add(cited_author)\n",
    "    \n",
    "    if unique_authors:\n",
    "        author_hop_data[source_author] = list(unique_authors)\n",
    "\n",
    "# Save the results\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(author_hop_data, f)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}\")\n",
    "print(f\"Total authors processed: {len(author_hop_data)}\")\n",
    "print(f\"Example mapping: {list(author_hop_data.items())[0] if author_hop_data else 'No results'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e09455",
   "metadata": {},
   "source": [
    "# Filters an author-level multi-hop citation influence map using two criteria:\n",
    "\n",
    "- **Threshold on number of reachable authors**: Keeps only authors citing more than `threshold` other authors.\n",
    "- **Fairness constraint**: Keeps only authors whose fairness score exceeds `fairness_threshold`.\n",
    "\n",
    "**Inputs:**\n",
    "- `multi_hop_author_name_11000.pkl`: Author-to-multi-hop-author map.\n",
    "- `fairness_values.pkl`: Dictionary mapping author names to fairness scores.\n",
    "\n",
    "**Output:**\n",
    "- `filtered_author_hop_k(10)(.1).pkl`: Filtered author influence map.\n",
    "\n",
    "**Useful Stats Printed:**\n",
    "- Total original authors.\n",
    "- Total authors after filtering.\n",
    "- Number of authors removed.\n",
    "- Example filtered entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d45acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Filtering with threshold = 10 and fairness >= 0.1 ===\n",
      "\n",
      "Processing results for multi_hop_author_name_11000.pkl:\n",
      "Original keys: 9967\n",
      "Filtered keys: 5897\n",
      "Keys removed: 4070\n",
      "\n",
      "Filtered data saved to: filtered_author_hop_k(10)(.1).pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "input_file = \"multi_hop_author_name_11000.pkl\"\n",
    "fairness_file = \"fairness_values.pkl\"\n",
    "output_file = \"filtered_author_hop_k(10)(.1).pkl\"\n",
    "threshold = 10\n",
    "fairness_threshold = 0.1\n",
    "\n",
    "print(f\"\\n=== Filtering with threshold = {threshold} and fairness >= {fairness_threshold} ===\")\n",
    "\n",
    "# Load multi-hop author data\n",
    "with open(input_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Load fairness values\n",
    "with open(fairness_file, \"rb\") as f:\n",
    "    fairness_scores = pickle.load(f)\n",
    "\n",
    "# Filter step: apply both conditions\n",
    "filtered_data = {\n",
    "    key: val for key, val in data.items()\n",
    "    if len(val) > threshold and fairness_scores.get(key, 0) > fairness_threshold\n",
    "}\n",
    "\n",
    "# Calculate stats\n",
    "original_count = len(data)\n",
    "filtered_count = len(filtered_data)\n",
    "removed_count = original_count - filtered_count\n",
    "\n",
    "# Save the filtered dictionary\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(filtered_data, f)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nProcessing results for {input_file}:\")\n",
    "print(f\"Original keys: {original_count}\")\n",
    "print(f\"Filtered keys: {filtered_count}\")\n",
    "print(f\"Keys removed: {removed_count}\")\n",
    "print(f\"\\nFiltered data saved to: {output_file}\")\n",
    "# print(f\"Example of kept entries: {list(filtered_data.items())[:2] if filtered_data else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9565eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique keys: 5897\n",
      "Total unique values: 71532\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your single .pkl file\n",
    "pkl_file = \"filtered_author_hop_k(10)(.1).pkl\"\n",
    "\n",
    "unique_keys = set()\n",
    "unique_values = set()\n",
    "\n",
    "with open(pkl_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "unique_keys.update(data.keys())\n",
    "for v_set in data.values():\n",
    "    unique_values.update(v_set)\n",
    "\n",
    "print(f\"Total unique keys: {len(unique_keys)}\")\n",
    "print(f\"Total unique values: {len(unique_values)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ea288",
   "metadata": {},
   "source": [
    "# Assigns a unique integer ID to each author in the filtered multi-hop influence graph.\n",
    "\n",
    "**Purpose:**\n",
    "- Converts author names (keys) into unique integer IDs starting from 1.\n",
    "\n",
    "**Inputs:**\n",
    "- `filtered_author_hop_k(10)(.1).pkl`: A pickle file containing a dictionary of author-to-author influence data.\n",
    "\n",
    "**Output:**\n",
    "- `author_keys_to_id_mapping.pkl`: A dictionary mapping author names to unique integer IDs.\n",
    "\n",
    "**Behavior:**\n",
    "- Loads the filtered author influence map.\n",
    "- Iterates over each author and assigns a unique ID.\n",
    "- Saves the mapping as a `author_keys_to_id_mapping.pkl` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8dec8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique author keys: 5897\n",
      "\n",
      " First 10 author-key-to-ID mappings:\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to single .pkl file\n",
    "pkl_file = \"filtered_author_hop_k(10)(.1).pkl\"\n",
    "\n",
    "author_to_id = {}\n",
    "current_id = 1\n",
    "\n",
    "# Load the single pickle file\n",
    "with open(pkl_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Assign unique IDs to each author (key)\n",
    "for key in data.keys():\n",
    "    if key not in author_to_id:\n",
    "        author_to_id[key] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "print(f\"Total unique author keys: {len(author_to_id)}\")\n",
    "\n",
    "# Save to file\n",
    "with open(\"author_keys_to_id_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(author_to_id, f)\n",
    "\n",
    "# Print first 10 entries\n",
    "print(\"\\n First 10 author-key-to-ID mappings:\")\n",
    "for i, (author, idx) in enumerate(author_to_id.items()):\n",
    "    # print(f\"{author}: {idx}\")\n",
    "    if idx==5921:\n",
    "        print(author)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73abd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (author, idx) in enumerate(author_to_id.items()):\n",
    "    # print(f\"{author}: {idx}\")\n",
    "    if idx==5921:\n",
    "        print(author)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e6508",
   "metadata": {},
   "source": [
    "# Assigns a unique integer ID to each author appearing in the values of a filtered multi-hop influence map.\n",
    "\n",
    "**Purpose:**\n",
    "- Converts all co-cited or cited author names (i.e., values in the dictionary) into unique integer IDs starting from 1.\n",
    "\n",
    "**Inputs:**\n",
    "- `filtered_author_hop_k(10)(.1).pkl`: A pickle file containing a dictionary where:\n",
    "  - Keys = author names (sources).\n",
    "  - Values = lists of reachable/cited author names (targets).\n",
    "\n",
    "**Output:**\n",
    "- `author_values_to_id_mapping.pkl`: A dictionary mapping each unique cited author (from the values) to a unique ID.\n",
    "\n",
    "**Behavior:**\n",
    "- Iterates over all lists of cited authors.\n",
    "- Assigns a unique ID to each cited author (ignores keys).\n",
    "- Saves the result as a `author_values_to_id_mapping.pkl` file.\n",
    "- Prints total number of unique authors.\n",
    "- Prints the first 10 author-to-ID mappings for verification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88fbf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique authors in values: 71532\n",
      "\n",
      "First 10 author-value-to-ID mappings:\n",
      "O. J. Dahl: 1\n",
      "Stephen P. Morse: 2\n",
      "Victor H. Yngve: 3\n",
      "Charles M. Eastman: 4\n",
      "Gregor V. Bochmann: 5\n",
      "James Blinn: 6\n",
      "John Adams: 7\n",
      "B. W. Arden: 8\n",
      "Edward H. Friend: 9\n",
      "L. C. Caruthers: 10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your single .pkl file\n",
    "pkl_file = \"filtered_author_hop_k(10)(.1).pkl\"\n",
    "\n",
    "author_value_to_id = {}\n",
    "current_id = 1\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pkl_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Map each author in the values to a unique ID\n",
    "for values in data.values():\n",
    "    for author in values:\n",
    "        if author not in author_value_to_id:\n",
    "            author_value_to_id[author] = current_id\n",
    "            current_id += 1\n",
    "\n",
    "print(f\"Total unique authors in values: {len(author_value_to_id)}\")\n",
    "\n",
    "# Save the mapping\n",
    "with open(\"author_values_to_id_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(author_value_to_id, f)\n",
    "\n",
    "# Print first 10 entries\n",
    "print(\"\\nFirst 10 author-value-to-ID mappings:\")\n",
    "for i, (author, idx) in enumerate(author_value_to_id.items()):\n",
    "    print(f\"{author}: {idx}\")\n",
    "    if i >= 9:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95a078",
   "metadata": {},
   "source": [
    "# Converts a filtered author-level multi-hop citation map from author names to unique integer IDs.\n",
    "\n",
    "**Purpose:**\n",
    "- Transforms the original mapping from author names to cited author names into a numeric ID-based dictionary.\n",
    "- Maps each author key to its unique ID, and each cited author in the values to their unique IDs.\n",
    "- Filters out self-citations (where key ID equals value ID).\n",
    "\n",
    "**Inputs:**\n",
    "- `filtered_author_hop_k(10)(.1).pkl`: Dictionary with author keys and lists of cited author names.\n",
    "- `author_keys_to_id_mapping.pkl`: Dictionary mapping author keys (names) to unique IDs.\n",
    "- `author_values_to_id_mapping.pkl`: Dictionary mapping cited author names (values) to unique IDs.\n",
    "\n",
    "**Output:**\n",
    "- `id_to_id_author_mapping.pkl`: Dictionary mapping author key IDs to lists of cited author IDs.\n",
    "\n",
    "**Behavior:**\n",
    "- For each author key, fetch its ID.\n",
    "- For each cited author in the values, fetch their ID if different from the keyâ€™s ID.\n",
    "- Store key ID mapped to the list of value IDs.\n",
    "- Prints the first 10 mappings as a sample for verification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615cc2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the filtered author hop file\n",
    "with open(\"filtered_author_hop_k(10)(.1).pkl\", \"rb\") as f:\n",
    "    filtered_data = pickle.load(f)\n",
    "\n",
    "# Load the author key to ID mapping\n",
    "with open(\"author_keys_to_id_mapping.pkl\", \"rb\") as f:\n",
    "    key_mapping = pickle.load(f)\n",
    "\n",
    "# Load the author value to ID mapping\n",
    "with open(\"author_values_to_id_mapping.pkl\", \"rb\") as f:\n",
    "    value_mapping = pickle.load(f)\n",
    "\n",
    "# Final dictionary to store: key_id -> list of value_ids\n",
    "id_to_id_dict = {}\n",
    "\n",
    "for author_key, author_values in filtered_data.items():\n",
    "    key_id = key_mapping.get(author_key)\n",
    "    \n",
    "    # Skip if the key author is not found in the mapping\n",
    "    if key_id is None:\n",
    "        continue\n",
    "\n",
    "    value_ids = []\n",
    "    for author in author_values:\n",
    "        # Skip if author is same as key author\n",
    "        if author == author_key:\n",
    "            continue\n",
    "        \n",
    "        value_id = value_mapping.get(author)\n",
    "        if value_id is not None and value_id != key_id:\n",
    "            value_ids.append(value_id)\n",
    "\n",
    "    id_to_id_dict[key_id] = value_ids\n",
    "\n",
    "# Print a sample\n",
    "# print(\"\\nðŸ”¹ First 10 key-ID to value-IDs mappings:\")\n",
    "# for i, (k, v) in enumerate(id_to_id_dict.items()):\n",
    "#     print(f\"{k}: {v}\")\n",
    "#     if i >= 9:\n",
    "#         break\n",
    "\n",
    "# Save the final dictionary\n",
    "with open(\"id_to_id_author_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_to_id_dict, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72d629",
   "metadata": {},
   "source": [
    "# Creates a mapping from author IDs to their fairness scores.\n",
    "\n",
    "**Purpose:**\n",
    "- Converts a mapping of author names to IDs into a mapping of author IDs to fairness scores.\n",
    "- Facilitates analyses where fairness metrics are needed by author ID rather than by name.\n",
    "\n",
    "**Inputs:**\n",
    "- `fairness_values.pkl`: Dictionary mapping author names to their fairness scores.\n",
    "- `author_keys_to_id_mapping.pkl`: Dictionary mapping author names to unique integer IDs.\n",
    "\n",
    "**Output:**\n",
    "- `id_fairness_mapping.pkl`: Dictionary mapping author IDs to their fairness scores.\n",
    "\n",
    "**How it works:**\n",
    "- Loads fairness values keyed by author names.\n",
    "- Loads author-to-ID mapping.\n",
    "- Creates a new dictionary mapping each author ID to the corresponding fairness score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the fairness values\n",
    "with open(\"fairness_values.pkl\", \"rb\") as f:\n",
    "    fairness_value = pickle.load(f)\n",
    "\n",
    "# Load the author to ID mapping\n",
    "with open(\"author_keys_to_id_mapping.pkl\", \"rb\") as f:\n",
    "    author_to_id_mapping = pickle.load(f)\n",
    "\n",
    "# Create the ID to fairness mapping\n",
    "id_fairness_mapping = {}\n",
    "\n",
    "for author, id in author_to_id_mapping.items():\n",
    "    # if author in fairness_value:\n",
    "    id_fairness_mapping[id] = fairness_value[author]\n",
    "\n",
    "# Save the new mapping to a file\n",
    "with open(\"id_fairness_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_fairness_mapping, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf4f91",
   "metadata": {},
   "source": [
    "# Generates combined vector representations and fairness scores for papers and their authors.\n",
    "\n",
    "**Purpose:**\n",
    "- Creates a one-hot vector representing each paper ID.\n",
    "- Creates a multi-hot vector representing authors linked to each paper.\n",
    "- Associates each paper with its fairness score.\n",
    "\n",
    "**Inputs:**\n",
    "- `id_fairness_mapping.pkl`: Mapping from author IDs to fairness scores.\n",
    "- `id_to_id_author_mapping.pkl`: Mapping from paper IDs to lists of author IDs.\n",
    "- `author_keys_to_id_mapping.pkl`: Mapping of paper authors (keys) to unique IDs (for one-hot vector size).\n",
    "- `author_values_to_id_mapping.pkl`: Mapping of cited authors (values) to unique IDs (for multi-hot vector size).\n",
    "\n",
    "**Outputs:**\n",
    "- `paper_author_vectors.pkl`: Dictionary keyed by paper ID, each value containing:\n",
    "  - `one_hot`: One-hot vector of the paper ID.\n",
    "  - `multi_hot`: Multi-hot vector representing all authors connected to that paper.\n",
    "  - `fairness`: Fairness score associated with the paper.\n",
    "\n",
    "**How it works:**\n",
    "- Loads all necessary mappings and fairness data.\n",
    "- For each paper:\n",
    "  - Creates a one-hot vector where only the paper ID index is set.\n",
    "  - Creates a multi-hot vector indicating all associated authors.\n",
    "  - Retrieves the fairness score for the paper.\n",
    "- Saves the combined data to a single pickle file for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819aefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load fairness values\n",
    "with open(\"id_fairness_mapping.pkl\", \"rb\") as f:\n",
    "    id_fairness_mapping = pickle.load(f)\n",
    "\n",
    "# Load mappings\n",
    "with open(\"id_to_id_author_mapping.pkl\", \"rb\") as f:\n",
    "    paper_to_authors = pickle.load(f)\n",
    "\n",
    "with open(\"author_keys_to_id_mapping.pkl\", \"rb\") as f:\n",
    "    onehot_mapping = pickle.load(f)\n",
    "\n",
    "with open(\"author_values_to_id_mapping.pkl\", \"rb\") as f:\n",
    "    multihot_mapping = pickle.load(f)\n",
    "\n",
    "\n",
    "# Vector sizes\n",
    "onehot_size = len(onehot_mapping)\n",
    "multihot_size = len(multihot_mapping)\n",
    "\n",
    "# Final dictionary to store both vectors and fairness\n",
    "paper_vectors = {}\n",
    "\n",
    "for paper_id, authors in paper_to_authors.items():\n",
    "    onehot_vec = np.zeros(onehot_size, dtype=np.uint8)\n",
    "    multihot_vec = np.zeros(multihot_size, dtype=np.uint8)\n",
    "\n",
    "    onehot_vec[paper_id - 1] = 1\n",
    "\n",
    "    author_fairness_values = []\n",
    "    for author_id in authors:\n",
    "        multihot_index = author_id - 1\n",
    "        multihot_vec[multihot_index] = 1\n",
    "\n",
    "        # Get fairness score if available\n",
    "        # fairness = id_fairness_mapping.get(author_id)\n",
    "        # if fairness is not None:\n",
    "        #     author_fairness_values.append(fairness)\n",
    "    fairness=id_fairness_mapping[paper_id]\n",
    "    # # Compute average fairness if any values are present\n",
    "    # if author_fairness_values:\n",
    "    #     avg_fairness = float(np.mean(author_fairness_values))\n",
    "    # else:\n",
    "    #     avg_fairness = 0.0  # Default if no known fairness scores\n",
    "\n",
    "    paper_vectors[paper_id] = {\n",
    "        \"one_hot\": onehot_vec,\n",
    "        \"multi_hot\": multihot_vec,\n",
    "        \"fairness\": fairness\n",
    "    }\n",
    "\n",
    "# Save to a single pickle file\n",
    "with open(\"paper_author_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(paper_vectors, f)\n",
    "\n",
    "print(\"Combined one-hot, multi-hot vectors and fairness values saved to paper_author_vectors.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4b67e",
   "metadata": {},
   "source": [
    "# Influence Count Extraction Script\n",
    "\n",
    "**Purpose:**\n",
    "This script calculates the number of influenced authors for each paper ID from the `id_to_id_author_mapping.pkl` file and stores the result in a pickle file named `influence_counts_before_after.pkl`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1be46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "influence_counts_before_after.pkl created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the mapping from 'id_to_id_author_mapping.pkl'\n",
    "with open('id_to_id_author_mapping.pkl', 'rb') as f:\n",
    "    id_to_author_mapping = pickle.load(f)\n",
    "\n",
    "# Create the count dictionary: key â†’ length of list of values\n",
    "influence_counts = {k: len(v) for k, v in id_to_author_mapping.items()}\n",
    "\n",
    "# Save the count dictionary to 'influence_counts_before_after.pkl'\n",
    "with open('influence_counts_before_after.pkl', 'wb') as f:\n",
    "    pickle.dump(influence_counts, f)\n",
    "\n",
    "print(\"influence_counts_before_after.pkl created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
